import asyncio
import concurrent.futures
import threading
import pandas as pd
from datetime import datetime, timezone
from utils.cache import Cache
from utils.logger import get_logger
from utils.data_validator import validate_features
from utils.api_guard import safe_get, pool_safe_get
from config import FINNHUB_API_KEY, USE_IBKR_DATA

logger = get_logger("FEATURE_ENGINE")
cache = Cache(ttl=60)

FINNHUB_CANDLE = "https://finnhub.io/api/v1/stock/candle"

# Thread pool pour les appels I/O bloquants (Finnhub HTTP, IBKR bars)
# max_workers=4 : suffisant pour paralléliser sans surcharger le réseau
_executor = concurrent.futures.ThreadPoolExecutor(
    max_workers=4,
    thread_name_prefix="feature-io"
)

# ib_insync n'est pas thread-safe : serialiser les appels get_bars()
# threading.Semaphore (pas asyncio) car la fonction tourne dans un thread
_ibkr_bar_lock = threading.Semaphore(1)

# IBKR connector (if enabled)
ibkr_connector = None

if USE_IBKR_DATA:
    try:
        from src.ibkr_connector import get_ibkr
        ibkr_connector = get_ibkr()
        if ibkr_connector and ibkr_connector.connected:
            logger.info("✅ IBKR connector active for market data")
        else:
            logger.warning("⚠️ IBKR not connected, falling back to Finnhub")
            ibkr_connector = None
    except Exception as e:
        logger.warning(f"⚠️ IBKR connector unavailable: {e}, using Finnhub")
        ibkr_connector = None

# ============================
# Helpers
# ============================

def clamp(x, low, high):
    return max(low, min(high, x))

def normalize_ratio(x, max_val):
    return clamp(x / max_val, 0, 1)

# ============================
# Finnhub candle circuit breaker
# After _FINNHUB_CANDLE_CB_THRESHOLD consecutive failures → skip Finnhub for _FINNHUB_CANDLE_CB_DURATION seconds
# ============================
import time as _time_mod
_finnhub_candle_failures = 0
_finnhub_candle_cb_until = 0.0
_FINNHUB_CANDLE_CB_THRESHOLD = 5    # open CB after 5 consecutive failures
_FINNHUB_CANDLE_CB_DURATION  = 1800  # 30 minutes

# ============================
# Price fetch (IBKR or Finnhub)
# ============================

def fetch_candles(ticker, resolution="1", lookback=120):
    """
    Fetch historical candles from IBKR (if available) or Finnhub
    
    Args:
        ticker: Stock symbol
        resolution: Candle size ("1" = 1 min)
        lookback: Minutes to look back
    
    Returns:
        DataFrame with OHLCV data
    """
    # Try IBKR first (if enabled and connected)
    if ibkr_connector and ibkr_connector.connected:
        try:
            # Convert lookback to IBKR duration
            if lookback <= 120:
                duration = '2 D'  # 2 days to ensure coverage
            elif lookback <= 1440:
                duration = '1 W'
            else:
                duration = '1 M'
            
            # Convert resolution to bar size
            bar_size = '1 min' if resolution == "1" else f"{resolution} mins"
            
            with _ibkr_bar_lock:
                df = ibkr_connector.get_bars(ticker, duration=duration, bar_size=bar_size)

            if df is not None and not df.empty:
                # Take only last 'lookback' bars
                df = df.tail(lookback)
                
                # Ensure required columns
                df = df.rename(columns={'date': 'timestamp'})
                
                logger.debug(f"✅ IBKR: Fetched {len(df)} bars for {ticker}")
                return df
            else:
                logger.debug(f"⚠️ IBKR returned no data for {ticker}, trying Finnhub")
        
        except Exception as e:
            logger.debug(f"⚠️ IBKR fetch failed for {ticker}: {e}, trying Finnhub")
    
    # Fallback to Finnhub (guarded by circuit breaker)
    global _finnhub_candle_failures, _finnhub_candle_cb_until
    if _time_mod.time() < _finnhub_candle_cb_until:
        return None  # circuit breaker open — skip Finnhub entirely

    try:
        now = int(datetime.now(timezone.utc).timestamp())
        start = now - lookback * 60

        params = {
            "symbol": ticker,
            "resolution": resolution,
            "from": start,
            "to": now,
            "token": FINNHUB_API_KEY
        }

        r = pool_safe_get(FINNHUB_CANDLE, params=params, provider="finnhub", task_type="CANDLES")
        data = r.json()

        if data.get("s") != "ok":
            raise ValueError(f"Finnhub candle no_data for {ticker}")

        df = pd.DataFrame({
            "open": data["o"],
            "high": data["h"],
            "low": data["l"],
            "close": data["c"],
            "volume": data["v"]
        })

        _finnhub_candle_failures = 0  # reset on success
        logger.debug(f"✅ Finnhub: Fetched {len(df)} bars for {ticker}")
        return df

    except Exception as e:
        _finnhub_candle_failures += 1
        if _finnhub_candle_failures >= _FINNHUB_CANDLE_CB_THRESHOLD:
            _finnhub_candle_cb_until = _time_mod.time() + _FINNHUB_CANDLE_CB_DURATION
            logger.warning(
                f"Finnhub candle circuit breaker OUVERT ({_finnhub_candle_failures} échecs) "
                f"— skip Finnhub candle pour 30 min"
            )
            _finnhub_candle_failures = 0  # reset counter after CB opens
        else:
            logger.debug(f"⚠️ Finnhub candle failed for {ticker} ({_finnhub_candle_failures}/{_FINNHUB_CANDLE_CB_THRESHOLD}): {e}")
        return None

# ============================
# Indicators (raw)
# ============================

def compute_vwap(df):
    pv = (df["close"] * df["volume"]).cumsum()
    vol = df["volume"].cumsum()
    # Guard against zero cumulative volume (empty bars or all-zero volume)
    return pv / vol.replace(0, float("nan"))

def raw_momentum(df, n=5):
    base = df["close"].iloc[-n]
    if base <= 0:
        return 0
    return (df["close"].iloc[-1] - base) / base

def raw_volume_spike(df):
    recent = df["volume"].iloc[-1]
    avg = df["volume"].iloc[:-1].mean()
    if avg <= 0:
        return 0
    return recent / avg

def raw_vwap_dev(df):
    vwap = compute_vwap(df).iloc[-1]
    if vwap <= 0:
        return 0
    return (df["close"].iloc[-1] - vwap) / vwap

def raw_volatility(df):
    return df["close"].pct_change().std()

# ============================
# Normalized features
# ============================

def momentum(df):
    raw = raw_momentum(df)
    return normalize_ratio(abs(raw), 0.15)   # 15% move = strong

def volume_spike(df):
    raw = raw_volume_spike(df)
    return normalize_ratio(raw, 8)            # cap at 8x avg volume

def vwap_deviation(df):
    raw = raw_vwap_dev(df)
    return clamp(raw, -0.05, 0.05) / 0.05     # normalize -1 → 1

def volatility(df):
    raw = raw_volatility(df)
    return normalize_ratio(raw, 0.05)         # 5% vol = high

# ============================
# Squeeze proxy (normalized)
# ============================

def squeeze_proxy(df):
    mom = abs(raw_momentum(df))
    vol = raw_volatility(df)
    if vol <= 0:
        return 0
    raw = mom / vol
    return normalize_ratio(raw, 10)           # cap extreme squeezes

# ============================
# Pattern flags
# ============================

def breakout_high(df, window=20):
    if len(df) < window + 1:
        return 0
    high = df["high"].iloc[-window:-1].max()
    return float(df["close"].iloc[-1] > high)

def strong_green(df):
    o = df["open"].iloc[-1]
    c = df["close"].iloc[-1]
    if o <= 0:
        return 0
    return float(c > o * 1.015)   # 1.5% candle

# ============================
# Feature builder
# ============================

def compute_features(ticker, include_advanced=True):
    """
    Compute features with optional advanced patterns
    
    Args:
        ticker: stock ticker
        include_advanced: if True, include pattern analyzer features
    
    Returns: dict of features
    """
    cached = cache.get(f"feat_{ticker}")
    if cached:
        return cached

    try:
        df = fetch_candles(ticker)

        # C9 FIX: Reduced from 20 to 5 bars minimum
        # 20 bars was too restrictive — many small-cap tickers have limited
        # intraday data especially in pre-market. 5 bars is enough for
        # momentum, volume spike, and VWAP calculations.
        if df is None or len(df) < 5:
            return None

        feats = {
            "momentum": momentum(df),
            "volume_spike": volume_spike(df),
            "vwap_dev": vwap_deviation(df),
            "volatility": volatility(df),
            "squeeze_proxy": squeeze_proxy(df),
            "breakout": breakout_high(df),
            "strong_green": strong_green(df),
        }
        
        # Add advanced pattern features if requested
        if include_advanced:
            try:
                from src.pattern_analyzer import (
                    bollinger_squeeze,
                    volume_accumulation,
                    higher_lows_pattern,
                    tight_consolidation,
                    momentum_acceleration
                )
                
                feats["bollinger_squeeze"] = bollinger_squeeze(df)
                feats["volume_accumulation"] = volume_accumulation(df)
                feats["higher_lows"] = higher_lows_pattern(df)
                feats["tight_consolidation"] = tight_consolidation(df)
                feats["momentum_accel"] = momentum_acceleration(df)
                
            except Exception as e:
                logger.warning(f"Advanced patterns error for {ticker}: {e}")
                # Continue without advanced features

        if not validate_features(feats):
            return None

        cache.set(f"feat_{ticker}", feats)
        return feats

    except Exception as e:
        logger.error(f"Feature error {ticker}: {e}")
        return None

# ============================
# Batch helper
# ============================

def compute_many(tickers, limit=None):
    results = {}

    for i, t in enumerate(tickers):
        if limit and i >= limit:
            break

        f = compute_features(t)
        if f:
            results[t] = f

    logger.info(f"Computed features for {len(results)} tickers")
    return results


# ============================
# Async versions (non-blocking)
# ============================

def _build_df_from_buffer(snapshots: list):
    """
    Build an approximate OHLCV DataFrame from TickerStateBuffer snapshots.

    S1-12 FIX: For streamé tickers, TickerStateBuffer already holds up to 120
    snapshots in RAM. Using them avoids a 2s IBKR get_bars() call per ticker.

    Limitation: L1 streaming gives last/bid/ask/volume — not true OHLC bars.
    Approximation: open=first_price, high=max, low=min, close=last, volume=last_volume.
    Acceptable for momentum/vwap/volatility. Less precise for breakout_high.
    """
    if not snapshots or len(snapshots) < 5:
        return None
    try:
        prices = [s.price for s in snapshots if s.price > 0]
        if len(prices) < 5:
            return None
        df = pd.DataFrame([{
            "open":   snapshots[0].price,
            "high":   max(s.price for s in snapshots),
            "low":    min(s.price for s in snapshots),
            "close":  s.price,
            "volume": s.volume,
        } for s in snapshots])
        if df["close"].isna().all() or (df["close"] == 0).all():
            return None
        return df
    except Exception:
        return None


async def fetch_candles_async(ticker, resolution="1", lookback=120):
    """
    Priority:
      1. TickerStateBuffer (0ms — streaming already in RAM for HOT tickers)
      2. IBKR get_bars() via thread pool (~2s fallback)
      3. Finnhub REST via thread pool (~500ms fallback)

    S1-12 FIX: Was always calling get_bars() even for tickers already streamé.
    """
    # --- Priority 1: TickerStateBuffer (streaming data, 0ms) ---
    try:
        from src.engines.ticker_state_buffer import get_ticker_state_buffer
        buf = get_ticker_state_buffer()
        snapshots = buf.get_snapshots(ticker, last_n=lookback)
        if len(snapshots) >= 5:
            df = _build_df_from_buffer(snapshots)
            if df is not None:
                logger.debug(f"Buffer hit: features for {ticker} ({len(snapshots)} snapshots)")
                return df
    except Exception as e:
        logger.debug(f"Buffer unavailable for {ticker}: {e}")

    # --- Priority 2 & 3: Network fallback (existing behaviour) ---
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        _executor,
        fetch_candles,
        ticker, resolution, lookback
    )


async def compute_features_async(ticker, include_advanced=True):
    """
    Async version of compute_features.
    Use from async contexts (e.g. process_ticker_v7) to avoid blocking the event loop.
    Falls back gracefully to sync compute_features if called outside an event loop.
    """
    cached = cache.get(f"feat_{ticker}")
    if cached:
        return cached

    try:
        df = await fetch_candles_async(ticker)

        if df is None or len(df) < 5:
            return None

        feats = {
            "momentum": momentum(df),
            "volume_spike": volume_spike(df),
            "vwap_dev": vwap_deviation(df),
            "volatility": volatility(df),
            "squeeze_proxy": squeeze_proxy(df),
            "breakout": breakout_high(df),
            "strong_green": strong_green(df),
        }

        if include_advanced:
            try:
                from src.pattern_analyzer import (
                    bollinger_squeeze,
                    volume_accumulation,
                    higher_lows_pattern,
                    tight_consolidation,
                    momentum_acceleration
                )

                feats["bollinger_squeeze"] = bollinger_squeeze(df)
                feats["volume_accumulation"] = volume_accumulation(df)
                feats["higher_lows"] = higher_lows_pattern(df)
                feats["tight_consolidation"] = tight_consolidation(df)
                feats["momentum_accel"] = momentum_acceleration(df)

            except Exception as e:
                logger.warning(f"Advanced patterns error for {ticker}: {e}")

        if not validate_features(feats):
            return None

        cache.set(f"feat_{ticker}", feats)
        return feats

    except Exception as e:
        logger.error(f"Feature error {ticker}: {e}")
        return None


if __name__ == "__main__":
    print(compute_features("AAPL"))
