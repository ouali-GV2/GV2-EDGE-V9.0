# ============================
# GV2-EDGE MAIN ENTRY POINT
# V7.0 Architecture Integration
# ============================

# Load environment variables FIRST (before any other imports)
from dotenv import load_dotenv
load_dotenv()

import time
import asyncio
import threading
import datetime
from typing import Dict, List, Optional

from utils.logger import get_logger
from utils.time_utils import is_premarket, is_market_open, is_market_closed, is_after_hours, market_session
from src.universe_loader import load_universe
from src.signal_logger import log_signal
from src.watch_list import get_watch_list, get_watch_upgrades

from alerts.telegram_alerts import send_signal_alert, send_system_alert
from monitoring.system_guardian import run_guardian
from weekly_deep_audit import run_weekly_audit_v2
from src.afterhours_scanner import run_afterhours_scanner
from daily_audit import run_daily_audit

# Config
from config import (
    USE_V7_ARCHITECTURE,
    DAILY_TRADE_LIMIT,
    MAX_POSITION_PCT,
    MANUAL_CAPITAL,
    ENABLE_PRE_HALT_ENGINE,
    ENABLE_IBKR_NEWS_TRIGGER,
    ENABLE_RISK_GUARD,
    ENABLE_MARKET_MEMORY,
    ENABLE_MULTI_RADAR,
    ENABLE_ACCELERATION_ENGINE,
    ENABLE_PRE_SPIKE_RADAR,
)

logger = get_logger("MAIN")

# ============================
# V7.0 ARCHITECTURE IMPORTS
# ============================

# Signal Producer (Layer 1 - Detection)
from src.engines.signal_producer import (
    SignalProducer,
    DetectionInput,
    get_signal_producer,
)
from src.models.signal_types import (
    SignalType,
    PreSpikeState,
    PreHaltState,
    UnifiedSignal,
)

# Order Computer (Layer 2 - Order Calculation)
from src.engines.order_computer import (
    OrderComputer,
    PortfolioContext,
    MarketContext,
    get_order_computer,
)

# Execution Gate (Layer 3 - Limits)
from src.engines.execution_gate import (
    ExecutionGate,
    AccountState,
    RiskFlags,
    MarketState,
    get_execution_gate,
)

# Risk Guard
from src.risk_guard import (
    get_unified_guard,
    RiskLevel,
    TradeAction,
)

# Market Memory (MRP/EP)
from src.market_memory import (
    enrich_signal_with_context,
    is_market_memory_stable,
    get_missed_tracker,
    MissReason,
)

# Multi-Radar Engine V9 (4 radars paralleles + confluence matrix)
from src.engines.multi_radar_engine import get_multi_radar_engine

# Pre-Halt Engine
from src.pre_halt_engine import (
    PreHaltEngine,
    get_pre_halt_engine,
)

# V8 Acceleration Engine (derivative-based anticipation)
from src.engines.acceleration_engine import get_acceleration_engine

# Pre-Spike Radar (precursor signals detection)
from src.pre_spike_radar import scan_pre_spike

# IBKR News Trigger
from src.ibkr_news_trigger import (
    IBKRNewsTrigger,
    get_ibkr_news_trigger as get_news_trigger,
)

# Monster Score (for detection input)
from src.scoring.monster_score import compute_monster_score

# Feature Engine (for market context)
from src.feature_engine import compute_features, compute_features_async

# IBKR connector for prices
from src.ibkr_connector import get_ibkr

# IBKR Streaming V9 (event-driven, ~10ms latency)
from src.ibkr_streaming import get_ibkr_streaming, start_ibkr_streaming

# Legacy imports (DEPRECATED - only used by edge_cycle() fallback when USE_V7_ARCHITECTURE=False)
# V9: Use SignalProducer + OrderComputer + MultiRadarEngine instead
from src.signal_engine import generate_signal
from src.portfolio_engine import process_signal

# Anticipation Engine (V5)
from src.anticipation_engine import (
    get_anticipation_engine,
    run_anticipation_scan,
    get_watch_early_signals,
    get_buy_signals,
    check_signal_upgrades,
    clear_expired_signals,
    get_engine_status,
    SignalLevel
)

# News Flow Screener (V5)
from src.news_flow_screener import (
    run_news_flow_screener,
    get_events_by_type,
    get_calendar_view
)

# Options Flow via IBKR (V5)
from src.options_flow_ibkr import (
    scan_options_flow,
    get_options_flow_score
)

# Extended Hours Quotes (V5)
from src.extended_hours_quotes import (
    get_extended_quote,
    scan_afterhours_gaps,
    scan_premarket_gaps,
    get_extended_hours_boost
)


# ============================
# V7.0 GLOBAL STATE
# ============================

class V7State:
    """V7.0 Architecture State Manager"""

    def __init__(self):
        self.trades_today = 0
        self.daily_pnl = 0.0
        self.last_reset_date = datetime.date.today()

        # Singleton instances
        self._producer: Optional[SignalProducer] = None
        self._computer: Optional[OrderComputer] = None
        self._gate: Optional[ExecutionGate] = None
        self._guard = None
        self._pre_halt: Optional[PreHaltEngine] = None
        self._news_trigger: Optional[IBKRNewsTrigger] = None
        self._missed_tracker = None
        self._streaming = None
        self._streaming_started = False
        self._finnhub_ws_started = False

    def check_day_rollover(self):
        """Reset daily counters if new day"""
        today = datetime.date.today()
        if today != self.last_reset_date:
            logger.info("New trading day - resetting counters")
            self.trades_today = 0
            self.daily_pnl = 0.0
            self.last_reset_date = today

    def record_trade(self, pnl: float = 0.0):
        """Record executed trade"""
        self.trades_today += 1
        self.daily_pnl += pnl

    @property
    def producer(self) -> SignalProducer:
        if self._producer is None:
            self._producer = get_signal_producer()
        return self._producer

    @property
    def computer(self) -> OrderComputer:
        if self._computer is None:
            self._computer = get_order_computer()
            self._computer.set_portfolio(PortfolioContext(
                total_capital=MANUAL_CAPITAL,
                available_cash=MANUAL_CAPITAL,
                risk_per_trade_pct=0.02,
                max_position_pct=MAX_POSITION_PCT
            ))
        return self._computer

    @property
    def gate(self) -> ExecutionGate:
        if self._gate is None:
            self._gate = get_execution_gate()
        return self._gate

    @property
    def guard(self):
        if self._guard is None and ENABLE_RISK_GUARD:
            self._guard = get_unified_guard()
        return self._guard

    @property
    def pre_halt(self) -> Optional[PreHaltEngine]:
        if self._pre_halt is None and ENABLE_PRE_HALT_ENGINE:
            self._pre_halt = get_pre_halt_engine()
        return self._pre_halt

    @property
    def news_trigger(self) -> Optional[IBKRNewsTrigger]:
        if self._news_trigger is None and ENABLE_IBKR_NEWS_TRIGGER:
            self._news_trigger = get_news_trigger()
        return self._news_trigger

    @property
    def missed_tracker(self):
        if self._missed_tracker is None and ENABLE_MARKET_MEMORY:
            self._missed_tracker = get_missed_tracker()
        return self._missed_tracker


# Global V7 state
_v7_state: Optional[V7State] = None


def get_v7_state() -> V7State:
    """Get V7.0 state singleton"""
    global _v7_state
    if _v7_state is None:
        _v7_state = V7State()
    return _v7_state


# ============================
# V7.0 SIGNAL PIPELINE
# ============================

async def process_ticker_v7(ticker: str, state: V7State) -> Optional[UnifiedSignal]:
    """
    V7.0 Signal Pipeline for single ticker

    Flow:
    1. SignalProducer -> Detection (never blocked)
    2. OrderComputer -> Order calculation (always computed)
    3. ExecutionGate -> Execution decision (limits applied here)

    Returns: UnifiedSignal with all layers populated
    """
    try:
        # === STEP 1: Get monster score and features ===
        score_data = compute_monster_score(ticker)

        if not score_data:
            return None

        monster_score = score_data.get("monster_score", 0)

        # Get features for market context (async â€” never blocks event loop)
        features = await compute_features_async(ticker)

        # Get current price â€” streaming first (10ms), IBKR poll fallback (2s)
        ibkr = get_ibkr()
        quote = None
        streaming_quote = None

        try:
            streaming = get_ibkr_streaming()
            if streaming.is_subscribed(ticker):
                # get_quote() reads from an in-memory dict â€” non-blocking
                streaming_quote = streaming.get_quote(ticker)
        except Exception as _sq_err:
            logger.debug(f"Streaming quote error {ticker}: {_sq_err}")

        if streaming_quote and streaming_quote.is_valid and streaming_quote.last > 0:
            # Use streaming data (10ms latency)
            quote = {
                "last": streaming_quote.last,
                "bid": streaming_quote.bid,
                "ask": streaming_quote.ask,
                "volume": streaming_quote.volume,
                "high": streaming_quote.high,
                "low": streaming_quote.low,
                "vwap": streaming_quote.vwap,
                "_source": "streaming",
            }
        elif ibkr and ibkr.connected:
            # Fallback to poll mode (2s latency)
            quote = ibkr.get_quote(ticker, use_cache=True)

        # Finnhub WS fallback â€” used when IBKR has no data (after-hours, pre-market
        # gaps, IBKR disconnect). WS accumulates real trades even outside RTH.
        if quote is None:
            try:
                _ws = get_v7_state()._finnhub_ws
                if _ws is not None:
                    _ws_state = _ws.get_ticker_state(ticker)
                    if _ws_state and _ws_state.last_price > 0:
                        quote = {
                            "last": _ws_state.last_price,
                            "volume": int(_ws_state.volume_5min),
                            "bid": 0.0,
                            "ask": 0.0,
                            "high": _ws_state.last_price,
                            "low": _ws_state.last_price,
                            "vwap": 0.0,
                            "_source": "finnhub_ws",
                        }
            except Exception as _ws_err:
                logger.debug(f"Finnhub WS quote error {ticker}: {_ws_err}")

        current_price = quote.get("last", 0) if quote else score_data.get("price", 0)

        if not current_price or current_price <= 0:
            return None

        # === STEP 1.5: ACCELERATION ENGINE V8 (reads from TickerStateBuffer) ===
        # S1-1 FIX: AccelerationEngine was imported but never called in this pipeline.
        # It was only invoked inside MultiRadarEngine.FlowRadar. Now wired here so
        # acceleration_state / volume_zscore / accumulation_score are always set.
        accel_score = None
        if ENABLE_ACCELERATION_ENGINE:
            try:
                accel_engine = get_acceleration_engine()
                accel_score = accel_engine.score(ticker)
            except Exception as e:
                logger.debug(f"AccelerationEngine error {ticker}: {e}")

        # === STEP 1.6: PRE-SPIKE RADAR ===
        # S1-8 FIX: Pre-Spike Radar was never invoked â€” pre_spike_state was always DORMANT.
        pre_spike_state = PreSpikeState.DORMANT
        pre_spike_score = 0.0
        if ENABLE_PRE_SPIKE_RADAR:
            try:
                volume_data = {
                    "current_volume": int(quote.get("volume", 0)) if quote else 0,
                    "historical_volumes": [features.get("avg_volume", features.get("volume", 1))] * 5 if features else [],
                    "avg_daily_volume": features.get("avg_volume", features.get("volume", 1)) if features else 1,
                }
                psr = scan_pre_spike(ticker, volume_data=volume_data)
                _alert_to_state = {
                    "HIGH": PreSpikeState.LAUNCHING,
                    "ELEVATED": PreSpikeState.READY,
                    "WATCH": PreSpikeState.CHARGING,
                    "NONE": PreSpikeState.DORMANT,
                }
                pre_spike_state = _alert_to_state.get(psr.alert_level, PreSpikeState.DORMANT)
                pre_spike_score = psr.pre_spike_probability
            except Exception as e:
                logger.debug(f"Pre-Spike Radar error {ticker}: {e}")

        # === STEP 2: Build detection input ===
        detection_input = DetectionInput(
            ticker=ticker,
            current_price=current_price,
            monster_score=monster_score,
            catalyst_score=score_data.get("components", {}).get("event", 0),
            pre_spike_state=pre_spike_state,
            pre_spike_score=pre_spike_score,
            catalyst_type=score_data.get("catalyst_type"),
            catalyst_confidence=score_data.get("catalyst_confidence", 0.5),
            volume_ratio=features.get("volume_ratio", 1.0) if features else 1.0,
            price_change_pct=features.get("price_change", 0) if features else 0,
            market_session="RTH" if is_market_open() else ("PRE" if is_premarket() else "POST"),
            # V8 AccelerationEngine fields (S1-1 FIX)
            acceleration_state=accel_score.state if accel_score else "DORMANT",
            acceleration_score=accel_score.acceleration_score if accel_score else 0.0,
            volume_zscore=accel_score.volume_zscore if accel_score else 0.0,
            accumulation_score=accel_score.accumulation_score if accel_score else 0.0,
            breakout_readiness=accel_score.breakout_readiness if accel_score else 0.0,
        )

        # === STEP 3: SIGNAL PRODUCER (Layer 1 - Detection) ===
        signal = await state.producer.detect(detection_input)

        if not signal.is_actionable():
            logger.debug(f"{ticker}: {signal.signal_type.value} (score: {monster_score:.2f})")
            return signal

        # === STEP 3b: MULTI-RADAR V9 (confluence enrichment â€” upgrade si consensus plus fort) ===
        if ENABLE_MULTI_RADAR:
            try:
                radar_engine = get_multi_radar_engine()
                radar_signal = await radar_engine.scan_ticker(
                    ticker, detection_input.market_session
                )
                signal.multi_radar_result = radar_signal.to_dict()

                # Upgrade signal_type si le consensus multi-radar est plus fort
                _radar_priority = {
                    "BUY_STRONG": 100, "BUY": 80,
                    "WATCH": 60, "EARLY_SIGNAL": 40, "NO_SIGNAL": 0,
                }
                _radar_to_enum = {
                    "BUY_STRONG": SignalType.BUY_STRONG,
                    "BUY": SignalType.BUY,
                    "WATCH": SignalType.WATCH,
                    "EARLY_SIGNAL": SignalType.EARLY_SIGNAL,
                }
                radar_prio = _radar_priority.get(radar_signal.signal_type, 0)
                current_prio = signal.signal_type.get_priority()
                if radar_prio > current_prio and radar_signal.signal_type in _radar_to_enum:
                    logger.info(
                        f"{ticker}: Multi-Radar upgrade "
                        f"{signal.signal_type.value} â†’ {radar_signal.signal_type} "
                        f"({radar_signal.agreement.value}, "
                        f"score={radar_signal.final_score:.2f})"
                    )
                    signal.signal_type = _radar_to_enum[radar_signal.signal_type]
            except Exception as e:
                logger.debug(f"Multi-Radar error {ticker}: {e}")

        # === STEP 4: PRE-HALT ENGINE (sets pre_halt_state) ===
        if state.pre_halt and ENABLE_PRE_HALT_ENGINE:
            try:
                halt_assessment = state.pre_halt.assess(
                    ticker=ticker,
                    current_price=current_price,
                    volatility=features.get("volatility", 0.05) if features else 0.05
                )
                signal.pre_halt_state = halt_assessment.pre_halt_state
            except Exception as e:
                logger.debug(f"Pre-halt assessment error {ticker}: {e}")

        # === STEP 5: MRP/EP ENRICHMENT (Market Memory) ===
        if ENABLE_MARKET_MEMORY:
            try:
                enrich_signal_with_context(signal)
            except Exception as e:
                logger.debug(f"MRP/EP enrichment error {ticker}: {e}")

        # === STEP 6: ORDER COMPUTER (Layer 2 - Always Computed) ===
        market_context = MarketContext(
            current_price=current_price,
            bid=quote.get("bid", current_price * 0.998) if quote else current_price * 0.998,
            ask=quote.get("ask", current_price * 1.002) if quote else current_price * 1.002,
            spread_pct=0.004,
            current_volume=int(quote.get("volume", 0)) if quote else 0,
            atr=features.get("atr", current_price * 0.05) if features else current_price * 0.05
        )

        signal = state.computer.compute_order(signal, market_context)

        # === STEP 7: RISK GUARD (get risk assessment) ===
        risk_flags = None
        if state.guard and ENABLE_RISK_GUARD:
            try:
                assessment = await state.guard.assess(
                    ticker=ticker,
                    current_price=current_price,
                    volatility=features.get("volatility") if features else None
                )

                risk_flags = RiskFlags(
                    ticker=ticker,
                    dilution_risk=assessment.dilution_profile.risk_level.value if assessment.dilution_profile else "LOW",
                    compliance_risk=assessment.compliance_profile.risk_level.value if assessment.compliance_profile else "LOW",
                    delisting_risk="HIGH" if assessment.compliance_profile and assessment.compliance_profile.has_delisting_risk else "LOW",
                    current_price=current_price,
                    is_penny_stock=current_price < 1.0,
                    badges=[str(f) for f in assessment.flags[:3]]
                )
            except Exception as e:
                logger.debug(f"Risk guard error {ticker}: {e}")

        # === STEP 8: EXECUTION GATE (Layer 3 - Limits) ===
        state.check_day_rollover()

        state.gate.set_account(AccountState(
            total_capital=MANUAL_CAPITAL,
            available_cash=MANUAL_CAPITAL,
            trades_today=state.trades_today,
            daily_trade_limit=DAILY_TRADE_LIMIT,
            daily_pnl=state.daily_pnl,
            broker_connected=ibkr.connected if ibkr else False
        ))

        state.gate.set_market(MarketState(
            is_market_open=is_market_open(),
            session=detection_input.market_session,
            is_holiday=False
        ))

        signal = state.gate.evaluate(signal, risk_flags)

        return signal

    except Exception as e:
        logger.error(f"V7 pipeline error {ticker}: {e}", exc_info=True)
        return None


def handle_signal_result(signal: UnifiedSignal, state: V7State):
    """
    Handle signal result - log, alert, track misses

    The signal is ALWAYS visible to the trader, even if blocked.
    """
    if not signal or not signal.is_actionable():
        return

    ticker = signal.ticker
    final_signal = signal.get_final_signal()

    logger.info(
        f"{final_signal}: {ticker} "
        f"(monster: {signal.monster_score:.2f}, pre-halt: {signal.pre_halt_state.value})"
    )

    # Check execution status
    if signal.is_executable():
        if signal.execution and signal.execution.size_multiplier < 1.0:
            logger.info(f"  Size reduced to {signal.execution.size_multiplier*100:.0f}%")

        # Build trade plan for logging and alerts
        if signal.proposed_order:
            order = signal.proposed_order
            trade_plan = {
                "ticker": ticker,
                "signal": signal.signal_type.value,
                "shares": order.size_shares,
                "entry": order.price_target,
                "stop": order.stop_loss,
                "monster_score": signal.monster_score,
                "confidence": order.confidence,
                "notes": f"V7.0 | {signal.pre_halt_state.value}"
            }

            log_signal(trade_plan)
            send_signal_alert(trade_plan)

            # S1-2 FIX: trades_today was never incremented â€” DAILY_TRADE_LIMIT never triggered.
            state.record_trade()

            logger.info(
                f"  TRADE PLAN: {order.size_shares} shares @ ${order.price_target:.2f} "
                f"(stop: ${order.stop_loss:.2f}) [trades today: {state.trades_today}/{DAILY_TRADE_LIMIT}]"
            )
    else:
        # Signal blocked - track the miss if enabled
        if signal.execution:
            block_reasons = [r.value for r in signal.execution.blocked_by]
            logger.info(f"  BLOCKED: {', '.join(block_reasons)}")

            # Track miss for Market Memory
            if state.missed_tracker and ENABLE_MARKET_MEMORY:
                try:
                    miss_reason = MissReason.OTHER
                    if "DAILY_TRADE_LIMIT" in block_reasons:
                        miss_reason = MissReason.DAILY_TRADE_LIMIT
                    elif "CAPITAL_INSUFFICIENT" in block_reasons:
                        miss_reason = MissReason.CAPITAL_INSUFFICIENT
                    elif "POSITION_LIMIT" in block_reasons:
                        miss_reason = MissReason.POSITION_LIMIT
                    elif "PRE_HALT_HIGH" in block_reasons:
                        miss_reason = MissReason.PRE_HALT_HIGH
                    elif any(r in block_reasons for r in ["DILUTION_HIGH", "COMPLIANCE_HIGH", "PENNY_STOCK_RISK"]):
                        miss_reason = MissReason.RISK_GUARD_BLOCK

                    state.missed_tracker.record_miss(
                        ticker=ticker,
                        signal_type=signal.signal_type.value,
                        signal_price=signal.proposed_order.price_target if signal.proposed_order else 0,
                        monster_score=signal.monster_score,
                        miss_reason=miss_reason
                    )
                except Exception as e:
                    logger.debug(f"Failed to track miss: {e}")

            send_signal_alert({
                "ticker": ticker,
                "signal": f"{signal.signal_type.value} (BLOCKED)",
                "monster_score": signal.monster_score,
                "notes": f"BLOCKED: {signal.execution.block_message}"
            })

    # Show MRP/EP context if active
    if signal.context_active:
        logger.info(
            f"  Context: MRP={signal.context_mrp:.0f}, EP={signal.context_ep:.0f} "
            f"(confidence: {signal.context_confidence:.0f}%)"
        )


# ============================
# EDGE CORE CYCLE V7.0
# ============================


# Persistent event loop â€” reused across all edge_cycle_v7() calls to avoid
# creating/destroying a new loop on every cycle (memory leak on long-running server)
_edge_loop: Optional[asyncio.AbstractEventLoop] = None
_edge_loop_lock = threading.Lock()


def _get_edge_loop() -> asyncio.AbstractEventLoop:
    """Get or create the persistent event loop for edge cycles."""
    global _edge_loop
    with _edge_loop_lock:
        if _edge_loop is None or _edge_loop.is_closed():
            _edge_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(_edge_loop)
        return _edge_loop


def edge_cycle_v7():
    """
    V7.0 Edge Cycle - Uses new unified signal architecture

    Detection -> Order -> Execution (with full visibility)
    """
    state = get_v7_state()
    state.check_day_rollover()

    universe = load_universe()

    if universe is None or universe.empty:
        logger.warning("Universe empty - skipping cycle")
        return

    logger.info(f"V7 CYCLE: Scanning {len(universe)} tickers")

    # Subscribe new tickers to streaming (keeps buffer fed for V8 engines)
    if state._streaming and state._streaming_started:
        try:
            all_tickers = universe["ticker"].tolist()
            subscribed_count = state._streaming.get_subscription_count()
            # Top up subscriptions if we have capacity (max 200)
            remaining = max(0, 200 - subscribed_count)
            if remaining > 0:
                new_tickers = [
                    t for t in all_tickers[:remaining + subscribed_count]
                    if not state._streaming.is_subscribed(t)
                ][:remaining]
                if new_tickers:
                    state._streaming.subscribe(new_tickers, priority="NORMAL")
        except Exception as e:
            logger.debug(f"Streaming subscribe error: {e}")

    # Process tickers using the persistent event loop (avoids memory leak)
    async def process_all():
        for _, row in universe.iterrows():
            ticker = row["ticker"]

            try:
                signal = await process_ticker_v7(ticker, state)

                if signal:
                    handle_signal_result(signal, state)

            except Exception as e:
                logger.error(f"V7 error on {ticker}: {e}", exc_info=True)

    loop = _get_edge_loop()
    loop.run_until_complete(process_all())

    # Log cycle summary
    producer_stats = state.producer.get_stats()
    gate_stats = state.gate.get_stats()

    logger.info(
        f"Cycle complete: {producer_stats['total_signals']} signals produced, "
        f"{gate_stats['signals_allowed']} allowed, {gate_stats['signals_blocked']} blocked"
    )


# ============================
# LEGACY EDGE CYCLE (Fallback)
# ============================

def edge_cycle():
    """
    Legacy cycle - Used when USE_V7_ARCHITECTURE = False
    """
    universe = load_universe()

    if universe is None or universe.empty:
        logger.warning("Universe empty - skipping cycle")
        return

    logger.info(f"Scanning {len(universe)} tickers")

    for _, row in universe.iterrows():

        ticker = row["ticker"]

        try:
            signal = generate_signal(ticker)

            if not signal or signal["signal"] == "HOLD":
                continue

            from src.ensemble_engine import apply_confluence
            signal = apply_confluence(signal)

            trade_plan = process_signal(signal)

            if not trade_plan:
                logger.warning(f"Could not create trade plan for {ticker}")
                continue

            log_signal(trade_plan)

            logger.info(
                f"TRADE PLAN: {trade_plan['signal']} {trade_plan['shares']} shares of {ticker} "
                f"@ ${trade_plan['entry']} (stop: ${trade_plan['stop']})"
            )

            send_signal_alert(trade_plan)

        except Exception as e:
            logger.error(f"EDGE error on {ticker}: {e}", exc_info=True)


# ============================
# IBKR NEWS TRIGGER HANDLER
# ============================

def process_ibkr_news(headlines: List[Dict]):
    """
    Process incoming IBKR news headlines for early alerts
    """
    if not ENABLE_IBKR_NEWS_TRIGGER:
        return

    state = get_v7_state()

    if not state.news_trigger:
        return

    for item in headlines:
        ticker = item.get("ticker")
        headline = item.get("headline", "")

        if not ticker or not headline:
            continue

        try:
            trigger = state.news_trigger.process_headline(ticker, headline)

            if trigger and trigger.should_alert:
                logger.info(
                    f"NEWS TRIGGER: {ticker} - {trigger.trigger_type.value} "
                    f"(level: {trigger.trigger_level.value})"
                )

                send_signal_alert({
                    "ticker": ticker,
                    "signal": f"NEWS_{trigger.trigger_level.value}",
                    "notes": f"NEWS: {headline[:100]}\n"
                            f"Type: {trigger.trigger_type.value}\n"
                            f"Actions: {trigger.recommended_actions}"
                })

        except Exception as e:
            logger.debug(f"News trigger error: {e}")


# ============================
# WEEKLY AUDIT SCHEDULER
# ============================

last_audit_day = None

def should_run_weekly_audit():
    now = datetime.datetime.now(datetime.timezone.utc)
    return now.weekday() == 4 and now.hour == 22  # Friday 22h UTC


# ============================
# UNIVERSE REBUILD SCHEDULER
# ============================

last_universe_rebuild_day = None

def should_rebuild_universe():
    """Rebuild universe every Sunday at 02:00 UTC (before watch list at 03:00)"""
    now = datetime.datetime.now(datetime.timezone.utc)
    return now.weekday() == 6 and now.hour == 2  # Sunday 02h UTC


# ============================
# DAILY AUDIT SCHEDULER
# ============================

last_daily_audit_day = None

def should_run_daily_audit():
    """Run daily audit at 20:30 UTC (after US market close)"""
    now = datetime.datetime.now(datetime.timezone.utc)
    return now.hour == 20 and now.minute >= 30


# ============================
# WATCH LIST SCHEDULER
# ============================

last_watch_list_day = None

def should_generate_watch_list():
    """Generate watch list once per day at 3 AM UTC (before PM)"""
    now = datetime.datetime.now(datetime.timezone.utc)
    return now.hour == 3  # 3 AM UTC


def generate_and_send_watch_list():
    """
    Generate watch list and send summary via Telegram
    """
    global last_watch_list_day

    now_date = datetime.datetime.now(datetime.timezone.utc).date()

    if now_date == last_watch_list_day:
        return

    logger.info("Generating daily WATCH list...")

    universe = load_universe()
    if universe is None or universe.empty:
        return

    tickers = universe["ticker"].tolist()
    watch_list = get_watch_list(universe_tickers=tickers)

    if not watch_list:
        logger.info("No WATCH signals today")
        return

    upgrades = get_watch_upgrades(watch_list)

    summary = f"DAILY WATCH LIST ({len(watch_list)} signals)\n\n"

    for watch in watch_list[:5]:
        summary += f"TARGET: {watch['ticker']} - {watch['event_type']}\n"
        summary += f"   {watch['days_to_event']} days | Impact: {watch['impact']:.2f}\n"
        summary += f"   {watch['reason']}\n\n"

    if upgrades:
        summary += f"\nUPGRADES TO BUY ({len(upgrades)}):\n"
        for upgrade in upgrades:
            summary += f"UPGRADED: {upgrade['ticker']} - {upgrade['reason']}\n"

    send_signal_alert({
        "ticker": "WATCH_LIST",
        "signal": "WATCH",
        "notes": summary
    })

    last_watch_list_day = now_date
    logger.info(f"Sent watch list: {len(watch_list)} signals, {len(upgrades)} upgrades")


# ============================
# MAIN LOOP
# ============================

# ============================
# WEEKEND SCHEDULER
# ============================

_weekend_thread = None
_weekend_thread_lock = threading.Lock()


def _run_weekend_scheduler():
    """Run WeekendScheduler in a daemon thread during weekends/holidays."""
    try:
        from src.weekend_mode.weekend_scheduler import get_weekend_scheduler
        send_system_alert(
            "\U0001f319 *Weekend Mode demarre*\n"
            "Scans batch + preparation watchlist lundi en cours..."
        )
        scheduler = get_weekend_scheduler()
        plan = scheduler.create_plan()
        total_jobs = sum(len(v) for v in plan.jobs_by_phase.values())
        logger.info(f"Weekend plan cree: {total_jobs} jobs planifies")

        asyncio.run(scheduler.execute_plan(plan))

        logger.info(f"Weekend scheduler termine: {plan.completed_jobs} OK, {plan.failed_jobs} echecs")
        send_system_alert(
            f"\u2705 *Weekend Mode termine*\n"
            f"Jobs: {plan.completed_jobs} reussis / {plan.failed_jobs} echecs"
        )
    except Exception as e:
        logger.error(f"Weekend scheduler failed: {e}", exc_info=True)
        send_system_alert(f"\u26a0\ufe0f *Weekend Mode echoue*: {e}")


def _ensure_weekend_scheduler():
    """Start the weekend scheduler thread if not already running."""
    global _weekend_thread
    with _weekend_thread_lock:
        if _weekend_thread is not None and _weekend_thread.is_alive():
            return  # Already running
        _weekend_thread = threading.Thread(
            target=_run_weekend_scheduler,
            name="weekend-scheduler",
            daemon=True,
        )
        _weekend_thread.start()
        logger.info("Weekend scheduler thread demarre")


def run_edge():
    global last_audit_day
    global last_daily_audit_day
    global last_universe_rebuild_day

    arch_mode = "V7.0" if USE_V7_ARCHITECTURE else "LEGACY"
    logger.info(f"GV2-EDGE LIVE ENGINE STARTED ({arch_mode} ARCHITECTURE)")

    if USE_V7_ARCHITECTURE:
        state = get_v7_state()
        logger.info(f"  V7 State initialized")
        logger.info(f"  Daily trade limit: {DAILY_TRADE_LIMIT}")
        logger.info(f"  Pre-Halt Engine: {'ENABLED' if ENABLE_PRE_HALT_ENGINE else 'DISABLED'}")
        logger.info(f"  Risk Guard: {'ENABLED' if ENABLE_RISK_GUARD else 'DISABLED'}")
        logger.info(f"  Market Memory: {'ENABLED' if ENABLE_MARKET_MEMORY else 'DISABLED'}")
        logger.info(f"  News Trigger: {'ENABLED' if ENABLE_IBKR_NEWS_TRIGGER else 'DISABLED'}")

        # Start IBKR Streaming V9 (event-driven, ~10ms)
        if not state._streaming_started:
            try:
                universe = load_universe()
                initial_tickers = universe["ticker"].tolist()[:50] if universe is not None else []

                streaming = start_ibkr_streaming(
                    tickers=initial_tickers,
                    connect_hot_queue=True,
                    connect_radar=True,
                )
                if streaming:
                    state._streaming = streaming
                    state._streaming_started = True
                    logger.info(f"  IBKR Streaming: STARTED ({len(initial_tickers)} initial subs)")
                else:
                    logger.warning("  IBKR Streaming: FAILED (falling back to poll mode)")
            except Exception as e:
                logger.warning(f"  IBKR Streaming: ERROR ({e}) â€” using poll mode")

        # Start Finnhub WebSocket Screener with dedicated FH_A key (WS-only, no REST rotation)
        # One persistent connection replaces ~3000 REST price-polling calls per cycle.
        if not state._finnhub_ws_started:
            try:
                import os as _os
                import threading as _threading
                from src.finnhub_ws_screener import get_finnhub_ws_screener
                ws_key = _os.environ.get("FINNHUB_KEY_A") or FINNHUB_API_KEY
                ws_screener = get_finnhub_ws_screener(api_key=ws_key)

                def _run_finnhub_ws():
                    import asyncio
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    loop.run_until_complete(ws_screener.start(tickers=initial_tickers))

                ws_thread = _threading.Thread(
                    target=_run_finnhub_ws,
                    daemon=True,
                    name="finnhub-ws",
                )
                ws_thread.start()
                state._finnhub_ws = ws_screener          # Store reference so process_ticker_v7 can query it
                state._finnhub_ws_started = True

                # Connect WS screener to TickerStateBuffer so every Finnhub trade
                # feeds AccelerationEngine / FlowRadar / SmallCapRadar (V8 engines).
                # Same pattern as ibkr_streaming.py:_feed_buffer().
                try:
                    from src.engines.ticker_state_buffer import get_ticker_state_buffer
                    ws_screener.connect_buffer(get_ticker_state_buffer())
                    logger.info("  Finnhub WS: Connected to TickerStateBuffer")
                except Exception as _buf_e:
                    logger.debug(f"  Finnhub WS: Buffer connect skipped: {_buf_e}")

                key_label = "FH_A (dedicated)" if _os.environ.get("FINNHUB_KEY_A") else "FH_MAIN (fallback)"
                logger.info(f"  Finnhub WS: STARTED ({len(initial_tickers)} subs, key={key_label})")
            except Exception as e:
                logger.warning(f"  Finnhub WS: FAILED ({e})")

        # S3-3 FIX: Initialize TickerStateBuffer baselines from universe data.
        # Without baselines, z-scores default to volume_mean=1 â†’ all tickers show
        # extreme z-scores on first real tick (e.g. 50000x vs baseline of 1).
        # Set per-ticker avg_volume from universe so AccelerationEngine gets
        # meaningful volume anomaly scores from tick 1.
        try:
            from src.engines.ticker_state_buffer import get_ticker_state_buffer
            buf = get_ticker_state_buffer()
            if universe is not None and not universe.empty:
                initialized = 0
                for _, row in universe.head(300).iterrows():
                    ticker = str(row.get("ticker") or row.get("symbol") or "").strip()
                    if not ticker:
                        continue
                    avg_vol = float(row.get("avg_volume") or row.get("avgVolume") or 500_000)
                    price = float(row.get("price") or row.get("last") or 5.0)
                    buf.set_baseline_raw(
                        ticker=ticker,
                        avg_price=max(price, 0.01),
                        price_std=max(price * 0.02, 0.01),    # 2% typical daily std
                        avg_volume=max(avg_vol, 1.0),
                        volume_std=max(avg_vol * 0.5, 1.0),  # 50% vol std is typical
                    )
                    initialized += 1
                logger.info(f"  TickerStateBuffer: {initialized} baselines initialized from universe")
        except Exception as e:
            logger.warning(f"  TickerStateBuffer baseline init failed: {e}")

    while True:
        try:
            now = datetime.datetime.now(datetime.timezone.utc).date()

            # ---- Weekly Universe Rebuild (Sunday 02:00 UTC) ----
            if should_rebuild_universe() and now != last_universe_rebuild_day:
                logger.info("Weekly universe rebuild starting...")
                try:
                    old_universe = load_universe()
                    old_count = len(old_universe) if old_universe is not None else 0
                    new_universe = load_universe(force_refresh=True)
                    new_count = len(new_universe) if new_universe is not None else 0
                    last_universe_rebuild_day = now
                    logger.info(f"Universe rebuilt: {old_count} â†’ {new_count} tickers")
                    send_system_alert(
                        f"ðŸ”„ *Universe Rebuilt* (weekly)\n"
                        f"Tickers: `{old_count}` â†’ `{new_count}`\n"
                        f"Source: Finnhub US (MIC-filtered, Common Stock only)"
                    )
                except Exception as e:
                    logger.error(f"Weekly universe rebuild failed: {e}", exc_info=True)

            # ---- Daily WATCH list (3 AM UTC) ----
            if should_generate_watch_list():
                logger.info("Generating daily WATCH list")
                generate_and_send_watch_list()

            # ---- Daily Audit (20:30 UTC - after market close) ----
            if should_run_daily_audit() and now != last_daily_audit_day:
                logger.info("Running Daily Audit")
                try:
                    run_daily_audit(send_telegram=True)
                    last_daily_audit_day = now
                except Exception as e:
                    logger.error(f"Daily Audit failed: {e}", exc_info=True)

            # ---- Weekly audit V2 ----
            if should_run_weekly_audit() and now != last_audit_day:
                logger.info("Running Weekly Deep Audit V2")
                run_weekly_audit_v2(days_back=7)
                last_audit_day = now

            # ---- After-hours catalyst scan ----
            if is_after_hours():
                logger.info("AFTER-HOURS session - ANTICIPATION MODE")

                universe = load_universe()
                tickers = universe["ticker"].tolist() if universe is not None else []

                # News Flow Screener
                try:
                    logger.info("Step 1: News Flow Screener...")
                    ticker_events = run_news_flow_screener(tickers, hours_back=6)

                    events_by_type = get_events_by_type(ticker_events)
                    for event_type, event_list in events_by_type.items():
                        if event_list:
                            tickers_with_event = [e['ticker'] for e in event_list[:5]]
                            logger.info(f"  {event_type}: {tickers_with_event}")

                except Exception as e:
                    logger.error(f"News Flow Screener failed: {e}", exc_info=True)
                    ticker_events = {}

                # Extended Hours Gaps
                try:
                    logger.info("Step 2: Extended Hours Gap Scan...")
                    ah_gaps = scan_afterhours_gaps(tickers[:100], min_gap=0.03)

                    for gap in ah_gaps[:10]:
                        logger.info(f"  GAP {gap.ticker}: gap={gap.gap_pct*100:+.1f}%, vol={gap.volume:,}")

                except Exception as e:
                    logger.error(f"Extended Hours scan failed: {e}", exc_info=True)
                    ah_gaps = []

                # Options Flow
                try:
                    high_priority = list(set(
                        list(ticker_events.keys())[:20] +
                        [g.ticker for g in ah_gaps[:10]]
                    ))

                    if high_priority:
                        logger.info(f"Step 3: Options Flow on {len(high_priority)} high-priority...")
                        options_signals = scan_options_flow(high_priority)

                        for ticker, signals in options_signals.items():
                            for sig in signals:
                                if sig.score >= 0.5:
                                    logger.info(f"  OPTIONS {ticker}: {sig.signal_type} (score: {sig.score:.2f})")

                except Exception as e:
                    logger.error(f"Options Flow scan failed: {e}", exc_info=True)

                # Anticipation Engine
                try:
                    logger.info("Step 4: Anticipation Engine...")
                    results = run_anticipation_scan(tickers, mode="afterhours")

                    for signal_dict in results.get("new_signals", []):
                        if signal_dict.get("signal_level") in ["BUY", "BUY_STRONG"]:
                            boost, boost_details = get_extended_hours_boost(signal_dict["ticker"])

                            send_signal_alert({
                                "ticker": signal_dict["ticker"],
                                "signal": signal_dict["signal_level"],
                                "monster_score": signal_dict.get("combined_score", 0),
                                "notes": f"ANTICIPATION ({signal_dict.get('urgency', 'N/A')})\n"
                                        f"Catalyst: {signal_dict.get('catalyst_type', 'N/A')}\n"
                                        f"AH Boost: +{boost:.2f}\n"
                                        f"{signal_dict.get('catalyst_summary', '')[:100]}"
                            })
                            logger.info(f"ANTICIPATION ALERT: {signal_dict['ticker']} - {signal_dict['signal_level']}")

                        elif signal_dict.get("signal_level") == "WATCH_EARLY":
                            logger.info(f"WATCH_EARLY: {signal_dict['ticker']} (score: {signal_dict.get('combined_score', 0):.2f})")

                    status = get_engine_status()
                    logger.info(f"Engine: {status['suspects_count']} suspects, {status['watch_signals_count']} signals")

                except Exception as e:
                    logger.error(f"Anticipation scan failed: {e}", exc_info=True)

                run_afterhours_scanner(tickers=tickers[:50])

                time.sleep(600)  # 10 min in after-hours

            # ---- Pre-market session ----
            elif is_premarket():
                logger.info("PRE-MARKET session - CONFIRMATION MODE")

                universe = load_universe()
                tickers = universe["ticker"].tolist() if universe is not None else []

                try:
                    results = run_anticipation_scan(tickers, mode="premarket")
                    upgrades = results.get("upgrades", [])

                    for upgrade in upgrades:
                        send_signal_alert({
                            "ticker": upgrade["ticker"],
                            "signal": upgrade.get("signal_level", "BUY"),
                            "monster_score": upgrade.get("combined_score", 0),
                            "notes": f"UPGRADED from WATCH_EARLY\n"
                                    f"Catalyst: {upgrade.get('catalyst_type', 'N/A')}\n"
                                    f"{upgrade.get('catalyst_summary', '')[:100]}"
                        })
                        logger.info(f"UPGRADE ALERT: {upgrade['ticker']} -> {upgrade.get('signal_level')}")

                    status = get_engine_status()
                    logger.info(f"PM Engine: {status['watch_signals_count']} signals active")

                except Exception as e:
                    logger.error(f"PM anticipation failed: {e}", exc_info=True)

                # Run V7 or legacy cycle
                if USE_V7_ARCHITECTURE:
                    edge_cycle_v7()
                else:
                    edge_cycle()

                time.sleep(300)  # every 5 min

            # ---- Regular market hours ----
            elif is_market_open():
                logger.info("REGULAR MARKET session")

                clear_expired_signals(max_age_hours=24)

                # Run V7 or legacy cycle
                if USE_V7_ARCHITECTURE:
                    edge_cycle_v7()
                else:
                    edge_cycle()

                time.sleep(180)  # every 3 min

            # ---- Market closed ----
            elif is_market_closed():
                session = market_session()  # "WEEKEND", "HOLIDAY", "CLOSED"
                if session in ("WEEKEND", "HOLIDAY"):
                    # Weekend/holiday: run batch scans + monday prep in background thread
                    _ensure_weekend_scheduler()
                    status = (
                        "running" if _weekend_thread and _weekend_thread.is_alive()
                        else "done"
                    )
                    logger.info(f"Market closed ({session}) - weekend mode [{status}]")
                    time.sleep(1800)  # 30 min cycle on weekends
                else:
                    # Overnight weekday: audits/watch-list handled above, just idle
                    logger.info("Market closed (overnight) - idle")
                    time.sleep(900)  # 15 min

            else:
                time.sleep(300)

        except Exception as e:
            logger.error(f"Main loop crash: {e}", exc_info=True)
            time.sleep(60)


# ============================
# SYSTEM GUARDIAN THREAD
# ============================

def start_guardian():
    run_guardian()


# ============================
# ENTRY POINT
# ============================

if __name__ == "__main__":

    logger.info("Booting GV2-EDGE")

    guardian_thread = threading.Thread(
        target=start_guardian,
        daemon=True
    )
    guardian_thread.start()

    run_edge()
